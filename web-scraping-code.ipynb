{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. PRE-WORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NASA MARS NEWS\n",
    "The following module of code will open NASA's Mars news site, and scrape its code for the latest Mars headline and summary text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target news url\n",
    "news_url = 'https://mars.nasa.gov/news'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use webdriver to load page with dynamic JS before scraping\n",
    "news_driver = webdriver.Chrome()\n",
    "news_driver.get(news_url)\n",
    "\n",
    "#sleep for one interval for JS to load before scrape\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape the HTML with BS\n",
    "news_soup = BeautifulSoup(news_driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quit browser after Soup is saved\n",
    "news_driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATEST MARS HEADLINE + SUMMARY:\n",
      "--------------------------------------------------\n",
      "- NASA's Treasure Map for Water Ice on Mars\n",
      "- A new study identifies frozen water just below the Martian surface, where astronauts could easily dig it up.\n"
     ]
    }
   ],
   "source": [
    "#find first headline + article summary\n",
    "news_headline = news_soup.find('div', class_='content_title')\n",
    "news_teaser = news_soup.find('div', class_='article_teaser_body')\n",
    "\n",
    "#save contents as specified variable\n",
    "news_title = news_headline.text\n",
    "news_p = news_teaser.text\n",
    "\n",
    "#confirmation\n",
    "print(\"LATEST MARS HEADLINE + SUMMARY:\")\n",
    "print('-' * 50)\n",
    "print(\"- \" + news_title)\n",
    "print(\"- \" + news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. JPL MARS IMAGES\n",
    "The following code module will visit the JPL space images site and scrape the location of the featured headline image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target jpl image page\n",
    "jpl_url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup chromedriver\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visit url\n",
    "browser.visit(jpl_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#push full image button\n",
    "jpl_button = browser.find_by_id('full_image')\n",
    "jpl_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup the html\n",
    "jpl_soup = BeautifulSoup(browser.html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close browser after scraping HTML\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate the image code\n",
    "jpl_image = jpl_soup.find('article', class_='carousel_item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate the tag containing the full-sized image location\n",
    "jpl_fullimage = jpl_image['style']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use regex to parse the tag string\n",
    "jpl_regex = re.search( 'spaceimages/images/wallpaper/\\w+-\\d+\\w\\d+.jpg', jpl_fullimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://jpl.nasa.gov/spaceimages/images/wallpaper/PIA20057-1920x1200.jpg\n"
     ]
    }
   ],
   "source": [
    "#final - image location confirmation\n",
    "featured_image_url = f'http://jpl.nasa.gov/{jpl_regex.group()}'\n",
    "print(featured_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MARS WEATHER\n",
    "The following code module will scrape the latest weather update tweet from the @marswxreport account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set target url\n",
    "weather_url = 'https://twitter.com/marswxreport'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#request html\n",
    "weather_request = requests.get(weather_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse through HTML with bs\n",
    "weather_soup = BeautifulSoup(weather_request.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find latest tweet\n",
    "mars_weather = weather_soup.find('p', class_='TweetTextSize').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use regex to find pic.twitter.com url\n",
    "tweet_tail = re.search('pic.twitter.com/\\w+', mars_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove tweet tail for final string\n",
    "mars_tweet = mars_weather.replace(tweet_tail.group(), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InSight sol 373 (2019-12-14) low -98.1ºC (-144.6ºF) high -20.1ºC (-4.1ºF)\n",
      "winds from the SW at 4.9 m/s (11.1 mph) gusting to 19.7 m/s (44.0 mph)\n",
      "pressure at 6.60 hPa\n"
     ]
    }
   ],
   "source": [
    "#confirmation\n",
    "print(mars_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MARS FACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set target url\n",
    "facts_url = 'https://space-facts.com/mars/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#request html\n",
    "facts_request = requests.get(facts_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the html with BS\n",
    "facts_soup = BeautifulSoup(facts_request.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate the facts table\n",
    "facts_table = facts_soup.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                      0                              1\n",
      "0  Equatorial Diameter:                       6,792 km\n",
      "1       Polar Diameter:                       6,752 km\n",
      "2                 Mass:  6.39 × 10^23 kg (0.11 Earths)\n",
      "3                Moons:            2 (Phobos & Deimos)\n",
      "4       Orbit Distance:       227,943,824 km (1.38 AU)\n",
      "5         Orbit Period:           687 days (1.9 years)\n",
      "6  Surface Temperature:                   -87 to -5 °C\n",
      "7         First Record:              2nd millennium BC\n",
      "8          Recorded By:           Egyptian astronomers]\n"
     ]
    }
   ],
   "source": [
    "#save table to  list\n",
    "facts_list = pd.read_html(str(facts_table))\n",
    "print(facts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. MARS HEMISPHERES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target hemisphere url\n",
    "hemi_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#request the hemi HTML and use bs to parse it\n",
    "hemi_request = requests.get(hemi_url)\n",
    "hemi_soup = BeautifulSoup(hemi_request.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find each link leading to the four hemisphere pages\n",
    "hemi_links = hemi_soup.find_all('a', class_='product-item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holder for scrape of anchor text containing hemisphere names\n",
    "hemi_names = []\n",
    "\n",
    "#loop through anchors for text\n",
    "for x in range(4):\n",
    "    hemi_split = hemi_links[x].text.split()\n",
    "    hemi_holder = []\n",
    "    \n",
    "    #separate \"enhanced\" from hemisphere names\n",
    "    for y in range(len(hemi_split)):\n",
    "        if hemi_split[y] != \"Enhanced\":\n",
    "            hemi_holder.append(hemi_split[y])\n",
    "            \n",
    "    hemi_names.append(\" \".join(hemi_holder))\n",
    "\n",
    "print(hemi_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#holder for image urls\n",
    "hemi_images = []\n",
    "\n",
    "#loop through individual pages and save url names\n",
    "for j in range(4):\n",
    "    d_url = \"https://astrogeology.usgs.gov\" + hemi_links[j]['href']\n",
    "    d_request = requests.get(d_url)\n",
    "    d_soup = BeautifulSoup(d_request.text, 'html.parser')\n",
    "    d_link = d_soup.find('a', text=\"Sample\")\n",
    "    hemi_images.append(d_link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(4):\n",
    "    h_url = hemi_images[k]\n",
    "    r = requests.get(h_url)\n",
    " \n",
    "    with open(hemi_names[k] + \".jpg\",\"wb\") as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine hemi names with image urls\n",
    "hemi_dict = []\n",
    "\n",
    "for z in range(4):\n",
    "    hemi_dict.append({'title': hemi_names[z], 'img_url': hemi_images[z]})\n",
    "    print({'title': hemi_names[z], 'img_url': hemi_images[z]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
